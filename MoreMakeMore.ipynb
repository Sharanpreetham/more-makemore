{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3c2e25-76da-4f66-87a7-3bb99f1a4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04480f1b-b8cc-4a27-b514-0848a4cba3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt','r').read().splitlines()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c046a3-e26b-4485-9f4d-667f808eb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =sorted(list(set(''.join(words))))\n",
    "sti={s:i+1 for i,s in enumerate(chars)} \n",
    "sti['.']=0\n",
    "its={s:i for i,s in sti.items()}\n",
    "vocab_size=len(its)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514359fb-1e15-4b55-ad00-a73d8e939921",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    words[i] = words[i] + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87722ab4-bd3b-4eb0-bf6f-9542eed36d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_size=3\n",
    "X,Y=[],[]\n",
    "for w in words:\n",
    "    context=[0]*cont_size\n",
    "    #print(w)\n",
    "    for ch in w:\n",
    "        ix=sti[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(its[i] for i in context),\"------->\",its[ix])\n",
    "        context  = context[1:]+[ix]\n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50135256-8d3f-4804-9db6-6b21ec6a46e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0],\n",
       "         [ 0,  0,  5],\n",
       "         [ 0,  5, 13],\n",
       "         ...,\n",
       "         [26, 26, 25],\n",
       "         [26, 25, 26],\n",
       "         [25, 26, 24]]),\n",
       " tensor([ 5, 13, 13,  ..., 26, 24,  0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca808de4-6368-4da1-8b61-83e46a760965",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_size=3\n",
    "\n",
    "def build_ds(words):\n",
    "    X,Y=[],[]\n",
    "    for w in words:\n",
    "        context=[0]*cont_size\n",
    "        #print(w)\n",
    "        for ch in w:\n",
    "            ix=sti[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(its[i] for i in context),\"------->\",its[ix])\n",
    "            context  = context[1:]+[ix]\n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    return X,Y\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "Xtr,Ytr = build_ds(words[:n1])\n",
    "Xva,Yva = build_ds(words[n1:n2])\n",
    "Xte,Yte = build_ds(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d24e14b-ec4c-4a9b-8394-56173e84c2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8598, 0.5601],\n",
       "        [0.3771, 0.0793],\n",
       "        [0.9796, 0.7679],\n",
       "        [0.9292, 0.3725],\n",
       "        [0.8713, 0.4423],\n",
       "        [0.5786, 0.4977],\n",
       "        [0.6446, 0.3617],\n",
       "        [0.6959, 0.8241],\n",
       "        [0.0623, 0.7495],\n",
       "        [0.1709, 0.7111],\n",
       "        [0.5432, 0.1298],\n",
       "        [0.9452, 0.3008],\n",
       "        [0.2042, 0.0314],\n",
       "        [0.4019, 0.8868],\n",
       "        [0.5151, 0.5126],\n",
       "        [0.4524, 0.1777],\n",
       "        [0.7004, 0.5837],\n",
       "        [0.5662, 0.8691],\n",
       "        [0.7786, 0.8137],\n",
       "        [0.5221, 0.8534],\n",
       "        [0.1046, 0.1352],\n",
       "        [0.5555, 0.2828],\n",
       "        [0.6722, 0.8154],\n",
       "        [0.2012, 0.1752],\n",
       "        [0.7905, 0.1230],\n",
       "        [0.2910, 0.7636],\n",
       "        [0.1158, 0.9782]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=torch.rand([27,2])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91bd2f3d-d05e-4fef-8ad2-a006c89f8df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape,Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9e5434-6bae-4713-9d01-3596c62de22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the current Model is:  11897\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "n_emb = 10 # the dimensionality of the lookup/embedding matrix\n",
    "n_hidden = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C=torch.rand((vocab_size,n_emb),                generator = g)\n",
    "w1=torch.randn((n_emb*cont_size,n_hidden),      generator = g) * ((5/3)/(30**0.5))#six because we give 3 inputs encoded in 2D space!!!read about it once more!!\n",
    "b1=torch.randn(n_hidden,                        generator = g) * 0.01\n",
    "w2=torch.randn((n_hidden,vocab_size),            generator = g) * 0.01\n",
    "b2=torch.randn(vocab_size,                      generator = g) * 0 \n",
    "bnmean_run =torch.zeros((1,n_hidden))\n",
    "bnstd_run = torch.ones((1,n_hidden)) \n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias=torch.zeros((1,n_hidden))\n",
    "parameters = [C,w1,b1,w2,b2]\n",
    "print(\"The number of parameters in the current Model is: \",sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "494d9dfe-c926-4c2d-bb72-ff07fe9e33a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2886751345948129"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cat(torch.unbind(emb,1),1)\n",
    "(5/3)/n_emb*cont_size**0.5 #read about gain function and kaming kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3950e9a-19bb-4588-853c-d2a4a87b8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e956c4d-54f1-49e1-87d6-811620c02824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lre=torch.linspace(-3,0,1000)\n",
    "#lrs=10**lre\n",
    "#lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf225f6-4549-4bac-b252-f96ca9df8fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tunable parameters are 47551\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self,fanin,fanout,bias=True):\n",
    "        self.weight = torch.randn((fanin,fanout),generator=g)/fanin**0.5\n",
    "        self.bias = torch.zeros(fanout) if bias else None\n",
    "    def __call__(self,x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out+=self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight]+([] if self.bias is None else [self.bias])\n",
    "class BatchNorm1d:\n",
    "    def __init__(self,dim,eps=1e-5,momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        #parameters(trained with backprop)\n",
    "        self.gamma = torch.ones(dim)#gain\n",
    "        self.beta = torch.zeros(dim)#bias\n",
    "        #buffers\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "\n",
    "    def __call__(self,x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0,keepdim = True)\n",
    "            xvar = x.var(0,keepdim = True,unbiased=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar+self.eps)\n",
    "        self.out = self.gamma*xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.gamma , self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self,x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C=torch.rand((vocab_size,n_emb),                generator = g)\n",
    "layers=[\n",
    "    Linear(n_emb * cont_size, n_hidden),BatchNorm1d(n_hidden), Tanh(),     \n",
    "    Linear(n_hidden,        n_hidden),  BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,        n_hidden),  BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,        n_hidden),  BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,        n_hidden),  BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,      vocab_size),  BatchNorm1d(vocab_size),\n",
    "]\n",
    "with torch.no_grad():\n",
    "    #layers[-1].weight*=0.1 #making last leyer less confident\n",
    "    layers[-1].gamma*=0.1\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer,Linear):\n",
    "            layer.weight*= 5/3\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(\"Total number of tunable parameters are\",sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45698b-fc84-47ac-8e7e-383cf461994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3018\n",
      "  10000/ 200000: 2.2993\n",
      "  20000/ 200000: 2.3659\n",
      "  30000/ 200000: 2.1266\n",
      "  40000/ 200000: 2.0976\n",
      "  50000/ 200000: 2.2632\n",
      "  60000/ 200000: 1.9850\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200001\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #constructing a mini batches so computation is easier\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,),generator = g)\n",
    "    Xe,Ye  = Xtr[ix],Ytr[ix]\n",
    "    #forward pass\n",
    "    emb = C[Xe]\n",
    "    x = emb.view(emb.shape[0],-1)######################!!!!!!!!!!!!!!111\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x,Ye)\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr=0.1 if i < 100000 else 0.01 # enabling step wise gradient decay\n",
    "    for p in parameters:\n",
    "        #p.data += -.1 * p.grad\n",
    "        p.data += -lr * p.grad   #gradient decay so just a small push for the model at the end\n",
    "\n",
    "    #tracking\n",
    "    if i  % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps-1:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda598f-dd81-437d-b6ae-a09dd35ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(figsize=(20,4))#width and height\n",
    "legends = []\n",
    "for i,layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer,Tanh):\n",
    "        t=layer.out\n",
    "        print(\"layer %d (%10s): mean%+.2f, std %.2f, saturated: %.2f%%\" % (i,layer.__class__.__name__,t.mean(),t.std(),(t.abs()>0.97).float().mean()*100))\n",
    "        hy,hx = torch.histogram(t,density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends);\n",
    "plt.title(\"Activation Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056518c6-76a8-4961-8887-6bcdcc618a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(figsize=(20,4))#width and height\n",
    "legends = []\n",
    "for i,layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer,Tanh):\n",
    "        t=layer.out.grad\n",
    "        print(\"layer %d (%10s): mean%+f, std %e, saturated: %.2f%%\" % (i,layer.__class__.__name__,t.mean(),t.std(),(t.abs()>0.97).float().mean()*100))\n",
    "        hy,hx = torch.histogram(t,density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends);\n",
    "plt.title(\"Activation Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a0221-bf21-4b44-b622-5deb84cfe68b",
   "metadata": {},
   "source": [
    "max_steps = 200001\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #constructing a mini batches so computation is easier\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,))\n",
    "    Xe,Ye  = Xtr[ix],Ytr[ix]\n",
    "    #forward pass\n",
    "    emb = C[Xe]\n",
    "    embcat = emb.view(emb.shape[0],-1)######################!!!!!!!!!!!!!!111\n",
    "    hpreact = embcat @ w1 +#b1  #we dont need bias here as we are subtracting bnmean again so its just useless\n",
    "    bnmeani=hpreact.mean(0,keepdim = True)\n",
    "    bnstdi=hpreact.std(0,keepdim = True)\n",
    "    hpreact = bngain*(hpreact-bnmeani)/bnstdi+bnbias\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "            bnmean_run = 0.99*bnmean_run+0.001*bnmeani\n",
    "            bnstd_run = 0.99*bnstd_run+0.001*bnstdi\n",
    "            #print(f\"Inside the 'with' block, loss.requires_grad is: {loss.requires_grad}\")\n",
    "    h=torch.tanh(hpreact) # why -1 e3 at 26:00\n",
    "    #print(f\"Inside the 'with' block, loss.requires_grad is: {loss.requires_grad}\")\n",
    "\n",
    "    logits=h @ w2 + b2\n",
    "    loss = F.cross_entropy(logits,Ye)\n",
    "    #print(loss.item())\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr=0.1 if i < 100000 else 0.01 # enabling step wise gradient decay\n",
    "    for p in parameters:\n",
    "        #p.data += -.1 * p.grad\n",
    "        p.data += -lr * p.grad   #gradient decay so just a small push for the model at the end\n",
    "\n",
    "    #tracking\n",
    "    if i  % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps-1:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e2f25-74e9-4347-ae58-74f8ba17df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpreact.std(0,keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafcaf0-89af-4a62-8705-813248a73756",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpreact.mean(0,keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae218af6-cddd-4797-9377-68d62559012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ad59e-4b5c-4b91-8e0c-e94100fb1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hpreact.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef00d7-728a-4be1-978f-c2a6fae05728",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49d82d-f519-46ff-aec7-9534f69a64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs()>0.99,cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f278f-48ce-43a2-9eb4-36219cc3dc72",
   "metadata": {},
   "source": [
    "\n",
    "# loss =-prob[torch.arange(67),Y].log().mean()\n",
    "# other way to calculate loss is through entropy function\n",
    "# reasons to call cross entropy : ...\n",
    "# loss = F.cross_entropy(logits,Y)\n",
    "# loss#!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce10d08-ee76-4a70-8178-4ca6e145465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ w1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af79938-871a-46f5-8e97-f8ea303915d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        \"train\":(Xtr,Ytr),\n",
    "        \"val\":(Xva,Yva),\n",
    "        \"test\":(Xte,Yte),\n",
    "    }[split]\n",
    "    emb = C[X]\n",
    "    x = emb.view(emb.shape[0],-1)######################!!!!!!!!!!!!!!111\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x,Y)\n",
    "    print(split,loss.item())\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae7603-cc44-447b-b0c8-0a7984b09e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    context = [0] * cont_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(1,-1)######################!!!!!!!!!!!!!!111\n",
    "        hpreact = embcat @ w1 +b1\n",
    "        h=torch.tanh(hpreact) # why -1 e3 at 26:00\n",
    "        logits=h @ w2 + b2\n",
    "        probs = F.softmax(logits,dim=1)\n",
    "        ix = torch.multinomial(probs,num_samples = 1,generator =g).item()\n",
    "        context = context[1:]+[ix]\n",
    "        out.append(ix)\n",
    "        if ix==0:\n",
    "            break\n",
    "    print(''.join(its[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b4c4f-8605-455e-a365-4311b8377199",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(n_emb * cont_size, n_hidden), nn.Tanh(),     \n",
    "    nn.Linear(n_hidden,        n_hidden),   nn.Tanh(),\n",
    "    nn.Linear(n_hidden,        n_hidden),   nn.Tanh(),\n",
    "    nn.Linear(n_hidden,        n_hidden),   nn.Tanh(),\n",
    "    nn.Linear(n_hidden,        n_hidden),   nn.Tanh(),\n",
    "    nn.Linear(n_hidden,      vocab_size),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcb39e-57d3-400e-9779-e2a0479abde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    words = open('names.txt', 'r').read().splitlines()\n",
    "except FileNotFoundError:\n",
    "    # Fallback if file not found (though user uploaded it)\n",
    "    words = ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "# 2. Build Dataset\n",
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_dataset(words)\n",
    "\n",
    "# 3. Initialize Model (Embedding size = 2 for visualization)\n",
    "n_embd = 2\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# 4. Train (Quick Run for visualization)\n",
    "# We need enough steps to separate the characters somewhat nicely\n",
    "steps = 5000 \n",
    "for i in range(steps):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]] \n",
    "    embcat = emb.view(emb.shape[0], -1) \n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = 0.1 if i < steps/2 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "# 5. Plot Embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200, alpha=0.6, edgecolors='white')\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='black', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.title('Learned Character Embeddings (2D Projection)', fontsize=16)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# Save\n",
    "plt.savefig('embeddings_plot.png', dpi=150, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
